# llm-based-metrics-meta-eval
The goal of this repository is to evaluate the performance of some LLMs for the task generating report for evaluation.

## Tools 
- https://github.com/explodinggradients/ragas

## Getting started

1. Create and activate the python virtual environment
   ```
   $ python -m venv .venv
   $ ./.venv/Scripts/activate
   ```

2. Install the requirements
   ```
   $ python -m pip install -r requirements.txt
   ```

3. Install the VSCode promptflow extension